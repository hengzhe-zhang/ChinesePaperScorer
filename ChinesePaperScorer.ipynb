{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Neural network based chinese paper scorer (Transformer+XGBoost)\n",
    "**This project is only used for education purposes, we should not use it in real-world.**\n",
    "\n",
    "As a teaching assistant, I always have to review a lot of papers, which makes me feel under great pressure. Thus, in order to relieve myself from repetitive work, I build this project and attempt to use machine learning to automate the scoring process.\n",
    "\n",
    "In contrast to the traditional neural network based scorer, this scorer is based on a traditional machine learning technique, XGBoost. The reason behind this is that XGBoost is easier to train compared with neural network in terms of resources and time consumption.\n",
    "\n",
    "The basic idea is rather simple, we can extract text from PDF documents, and then use Transformer to extract the document embedding from the documents. After that, we can use XGBoost to train based on document embedding and document scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Document extraction\n",
    "Due to the privacy policy, in this project, I will not use any private papers. Thus, I downloaded several papers from CNKI as training data. I marked the paper downloaded from top-notch Chinese journal as 100 points and other papers as 60 points.\n",
    "\n",
    "Because all documents are PDF format, we should convert those files to text first. This is rather simple by using the package \"textract\". It should be noted that the current implementation of textract may raise errors when converting some documents. Thus, I slightly modified the source code to avoid that issue. The modified version of \"textract\" can be seen in the Github repository of this project."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:12<00:00,  1.54it/s]\n",
      "100%|██████████| 20/20 [00:09<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total error 0\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import textract\n",
    "\n",
    "def file_to_text(pdf):\n",
    "    text = textract.process(pdf)\n",
    "    return text.decode('utf8')\n",
    "\n",
    "\n",
    "def data_preparation(train):\n",
    "    d = []\n",
    "    error = 0\n",
    "    for i in ['A','B']:\n",
    "        path = f'pdf/{\"train\" if train else \"test\"}/{i}'\n",
    "        for f in tqdm(os.listdir(path)):\n",
    "            score = 100 if i=='A' else 60\n",
    "            try:\n",
    "                text = file_to_text(os.path.join(path, f))\n",
    "            except Exception as e:\n",
    "                error += 1\n",
    "                print(e)\n",
    "                continue\n",
    "            d.append({\n",
    "                'text': text,\n",
    "                'score': score\n",
    "            })\n",
    "    frame = pd.DataFrame(d)\n",
    "    print('Total error', error)\n",
    "    return frame\n",
    "\n",
    "train_data=data_preparation(train=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model training\n",
    "After extracting the documents, we get a table that contains training text and training score. Then, we can use the sentence transformer to encode our text. The theory of transformer is not the focus point of this paper, thus we will omit it. The only thing we need to remember is that the Transformer can encode a paragraph to a fixed length vector by using the pre-trained neural network.\n",
    "\n",
    "In the final step, we only need to fit the embedded text and the corresponding training score by using XGBoost. We use PCA to reduce the dimension of the embedding vector because our training data is too small."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from harvesttext import HarvestText\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "def batch_embedding(corpus):\n",
    "    ht0 = HarvestText()\n",
    "    model = SentenceTransformer('distiluse-base-multilingual-cased')\n",
    "    corpus_embeddings = model.encode(list(map(lambda x: ht0.clean_text(x), corpus.tolist())))\n",
    "    return corpus_embeddings\n",
    "\n",
    "def train_task(data):\n",
    "    print('data', data.shape)\n",
    "    embedding = batch_embedding(data['text'])\n",
    "    pipe = Pipeline([('PCA', PCA(n_components=24)), ('xgb', XGBRegressor())])\n",
    "    score = cross_validate(pipe, embedding, data['score'], scoring='neg_mean_absolute_error', cv=3)\n",
    "    print(score)\n",
    "    pipe.fit(embedding, data['score'])\n",
    "    return pipe"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data (40, 2)\n",
      "{'fit_time': array([0.10973072, 0.10060215, 0.1289463 ]), 'score_time': array([0.00138736, 0.00129271, 0.00121737]), 'test_score': array([-3.50952148e-04, -1.92788931e-04, -2.77989003e+01])}\n"
     ]
    }
   ],
   "source": [
    "mode=train_task(train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model validation\n",
    "In order to validate our model, next, we will apply our model on the test data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:07<00:00,  2.70it/s]\n",
      "100%|██████████| 20/20 [00:07<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total error 0\n"
     ]
    }
   ],
   "source": [
    "test_data=data_preparation(train=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we find that the model can generalize well on the testing data. It should be noted that in this project, we set the same score for those papers that were published in the same journal. For a more accurate model, we also can manually mark the score for each paper, and then we can get a more accurate score for each new paper."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0001313209533693\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "embedding = batch_embedding(test_data['text'])\n",
    "print(mean_absolute_error(test_data['score'],mode.predict(embedding)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}